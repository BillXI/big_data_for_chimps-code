local:

  images:
    # Community-accepted starting points we
    'library/debian:stable':           { 'external': true, est_size: '154.7 MB' }
    'library/debian:latest':           { 'external': true, est_size: '154.7 MB' }
    'library/debian:jessie':           { 'external': true, est_size: '154.7 MB' }
    'library/java:7-jdk':              { 'external': true, est_size: '449.8 MB' }
    'radial/busyboxplus:latest':       { 'external': true, est_size: '' }
    'blalor/docker-hosts:latest':      { 'external': true, est_size: '' }
    'phusion/baseimage:0.9.15':        { 'external': true, est_size: '' }
    'radial/busyboxplus:latest':       { 'external': true, est_size: '' }
    'dockerfile/elasticsearch:latest': { 'external': true, est_size: '' }
    #
    'bd4c/baseimage:latest':      { }
    'bd4c/hadoop_base:latest':    { }
    'bd4c/volume_boxer:latest':   { }
    'bd4c/hadoop_nn:latest':      { }
    'bd4c/hadoop_rm:latest':      { }
    'bd4c/hadoop_snn:latest':     { }
    'bd4c/hadoop_worker:latest':  { }
    'bd4c/hadoop_lounge:latest':  { }
    'bd4c/lipstick:latest':       { }
    #
    'bd4c/data_gold:latest':      { 'kind': 'data' }
    'bd4c/data_hdfs0:latest':     { 'kind': 'data' }
    'bd4c/data_hue:latest':       { 'kind': 'data' }
    'bd4c/data_nn:latest':        { 'kind': 'data' }
    'bd4c/data_outd:latest':      { 'kind': 'data' }
    'bd4c/home_chimpy:latest':    { 'kind': 'data' }
    'bd4c/home_chimpy:v0.01':     { 'kind': 'data' }
    
  # clusters
  clusters:

    # ===========================================================================
    #
    # Helper images
    #
    helpers:
      host_filer:
        doc:         "Manages /etc/hosts on all containers, allowing discovery"
        image_name:  "blalor/docker-hosts:latest"
        entry_args:  ["/srv/hosts"]
        volumes:
          - "/var/lib/docker/hosts:/srv/hosts"
          - "/var/run/docker.sock:/var/run/docker.sock"
        hostname:     "host-filer"

      es:
        doc:         "Elasticsearch server for Lipstick"
        image_name:  "dockerfile/elasticsearch:latest"
        hostname:    "es"
        ports:       [ "9200~api", "9300~transport" ]

    # ===========================================================================
    #
    # Data
    #

    data:
      data_gold:   { volumes: ["/data/gold:/data/gold"],               entry_args: ["pushpull", "/data/gold"],        image_name: "bd4c/data_gold", _type: "rucker.manifest.data_container"   }
      data_outd:   { volumes: ["/data/outd:/data/outd"],               entry_args: ["pushpull", "/data/outd"],        image_name: "bd4c/data_outd", _type: "rucker.manifest.data_container"   }
      data_hue:    { volumes: ["/bulk/hadoop/hue:/bulk/hadoop/hue"],   entry_args: ["pushpull", "/bulk/hadoop/hue"],  image_name: "bd4c/data_hue", _type: "rucker.manifest.data_container"    }
      data_nn:     { volumes: ["/bulk/hadoop/name:/bulk/hadoop/name"], entry_args: ["pushpull", "/bulk/hadoop/name"], image_name: "bd4c/data_nn", _type: "rucker.manifest.data_container"     }
      data_hdfs0:  { volumes: ["/bulk/hadoop/hdfs:/bulk/hadoop/hdfs"], entry_args: ["pushpull", "/bulk/hadoop/hdfs"], image_name: "bd4c/data_hdfs0", _type: "rucker.manifest.data_container"  }
      home_chimpy: { volumes: ["/home/chimpy:/home/chimpy"],           entry_args: ["pushpull", "/home/chimpy"],      image_name: "bd4c/home_chimpy", _type: "rucker.manifest.data_container" }

    # ===========================================================================
    #
    # Hadoop machines
    #
    # Remember that these must be in order you want them started
    #
    hadoop:

      lipstick:
        doc:         "Lipstick server for pig jobs pretty"
        image_name:  "bd4c/lipstick:latest"
        hostname:    "lipstick"
        ports:       [ "10001:9292~ui" ]
        links:       [ "es:es" ]
        volumes_from:
          - home_chimpy

      hadoop_nn:
        image_name:   "bd4c/hadoop_nn"
        hostname:     "nn"
        # console http 50070; ipc 8020
        ports:         [ "9522:22~ssh", "50070:50070~view", "8020:8020~ipc" ]
        volumes_from:
          - data_nn
        volumes:
           # dynamic /etc/hosts file
          - "/var/lib/docker/hosts:/etc/hosts:ro"
          - "/bulk/hadoop/log:/bulk/hadoop/log:rw"

      hadoop_snn:
        image_name:   "bd4c/hadoop_snn"
        hostname:     "snn"
        # checkpoint api http 50090
        ports:         [ "9422:22~ssh", "50090~ipc" ]
        links:
          - "hadoop_nn:nn"
        volumes:
          - "/var/lib/docker/hosts:/etc/hosts:ro"
          - "/bulk/hadoop/log:/bulk/hadoop/log:rw"

      hadoop_rm:
        image_name:   "bd4c/hadoop_rm"
        hostname:     "rm"
        # rsrc mgr: console http 8088; scheduler 8030; tracker 8031; ipc 8032; admin 8033, shuffle 13562;
        # history:  console 19888 / https 19890, ipc 10020, admin 10033
        ports:         [ "9322:22~ssh", "8088:8088~view", "8030:8030~sched", "8031:8031~tracker", "8032:8032~ipc", "8033:8033~yarnad", "13562:13562~shuffle", "19888:19888~histview", "10020:10020~ipc", "10033:10033~histad" ]
        links:
          - "hadoop_nn:nn"
        volumes:
          - "/var/lib/docker/hosts:/etc/hosts:ro"
          - "/bulk/hadoop/log:/bulk/hadoop/log:rw"

      # hadoop_jt:
      #   image_name:   "bd4c/hadoop_jt"
      #   hostname:     "jt"
      #   # 
      #   ports:         [ "9222:22~ssh", "50030:50030~view", "8021:8021~ipc", "9290:9290~thrift" ]
      #   links:
      #     - "hadoop_nn:nn"
      #   volumes:
      #     - "/var/lib/docker/hosts:/etc/hosts:ro"
      #     - "/bulk/hadoop/log:/bulk/hadoop/log:rw"

      #
      # TODO: We have hardcoded port mappings to the first docker host, which is
      # convenient. It also however forces us to have a one-worker-only cluster.
      #
      hadoop_worker00:
        image_name:   "bd4c/hadoop_worker"
        # node mgr:  console http 8042, localizer 8040, ipc 8041
        # data node: console http 50075, ipc 50020, xceiver 50010
        ports:         [ "9122:22~ssh", "8042:8042~nmview", "50075:50075~dnview", "50060:50060~tt", "8040~lclzr", "8041~nmipc", "50020~dnipc", "50010~dnxceiver" ]
        links:
          - "hadoop_nn:nn"
          - "hadoop_rm:rm"
          # - "hadoop_jt:jt"
        volumes:
          - "/var/lib/docker/hosts:/etc/hosts:ro"
          - "/bulk/hadoop/log:/bulk/hadoop/log:rw"
        volumes_from:
          - "data_hdfs0"
        hostname:     "worker00"

      #
      # Running hue on the lounge so that there's only one URL users need to learn.
      #
      hadoop_lounge:
        image_name:   "bd4c/hadoop_lounge"
        hostname:     "lounge"
        ports:         [ "9022:22~ssh", "10000:10000~hue" ]
        volumes:
          - "/bulk/hadoop/log:/bulk/hadoop/log:rw"
          - "/Users:/Users:rw"
        volumes_from:
          - "data_gold"
          - "data_outd"
          - "home_chimpy"
          - "data_hue"
        links:
          - "hadoop_nn:nn"
          - "hadoop_rm:rm"
          - "lipstick:lipstick"
          - "hadoop_worker00:worker00"
          # - "hadoop_jt:jt"

  # extra ports to open up to allow expansion
  extra_ports:
    - 9622~ssh
    - 9722~ssh
    - 9822~ssh
    - 10002~util
    - 10003~util
    - 10004~util
    - 10005~util
    - 10006~util
    - 10007~util
    - 10008~util
    - 10009~util
