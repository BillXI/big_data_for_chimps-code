local:
  provider:             'docker'
  
  nodes:
    control:
      node_type:        't2.medium'
      region:           'us-west-2'
      instances:        1
      containers:
        # - hostname_bot
        - es
        #
        - data_nn
        - data_hue
        - data_gold
        - data_outd
        - home_chimpy
        #
        - nn
        - rm
        - snn
        - hue
        - lounge

    hero:
      node_type:        't2.medium'
      region:           'us-west-2'
      instances:        1
      containers:
        - worker00
        - data_hdfs0

    grunts:
      node_type:        't2.medium'
      region:           'us-west-2'
      instances:        0
      containers:
        - grunt

  images:
    # Community-accepted starting points we
    # debian:            { repo_tag: 'library/debian:stable',              external: true, est_size: '154.7 MB' }
    # debian_stable:     { repo_tag: 'library/debian:stable',              external: true, est_size: '154.7 MB' }
    # debian_latest:     { repo_tag: 'library/debian:latest',              external: true, est_size: '154.7 MB' }
    # debian_jessie:     { repo_tag: 'library/debian:jessie',              external: true, est_size: '154.7 MB' }
    java_7:            { repo_tag: 'library/java:7-jdk',                 external: true, est_size: '449.8 MB' }
    busyboxplus:       { repo_tag: 'radial/busyboxplus:latest',          external: true, est_size: '' }
    docker_hosts:      { repo_tag: 'blalor/docker-hosts:latest',         external: true, est_size: '' }
    baseimage:         { repo_tag: 'phusion/baseimage:0.9.15',           external: true, est_size: '' }
    elasticsearch:     { repo_tag: 'dockerfile/elasticsearch:latest',    external: true, est_size: '' }
    #
    baseimage:         { repo_tag: 'tutum.co/mrflip/baseimage:latest',  }
    volume_boxer:      { repo_tag: 'mrflip/volume_boxer:latest',  }
    #
    hadoop_base:       { repo_tag: 'tutum.co/mrflip/hadoop_base:latest',  }
    hadoop_nn:         { repo_tag: 'tutum.co/mrflip/hadoop_nn:latest',  }
    hadoop_rm:         { repo_tag: 'tutum.co/mrflip/hadoop_rm:latest',  }
    hadoop_snn:        { repo_tag: 'tutum.co/mrflip/hadoop_snn:latest',  }
    hadoop_hue:        { repo_tag: 'tutum.co/mrflip/hadoop_hue:latest',  }
    hadoop_worker:     { repo_tag: 'tutum.co/mrflip/hadoop_worker:latest',  }
    hadoop_lounge:     { repo_tag: 'tutum.co/mrflip/hadoop_lounge:latest',  }
    # lipstick:        { repo_tag: 'tutum.co/mrflip/lipstick:latest',  }
    #
    data_gold:         { repo_tag: 'mrflip/data_gold:latest',   'kind': 'data' }
    data_hdfs0:        { repo_tag: 'mrflip/data_hdfs0:latest',  'kind': 'data' }
    data_hue:          { repo_tag: 'mrflip/data_hue:latest',    'kind': 'data' }
    data_nn:           { repo_tag: 'mrflip/data_nn:latest',     'kind': 'data' }
    data_outd:         { repo_tag: 'mrflip/data_outd:latest',   'kind': 'data' }
    home_chimpy:       { repo_tag: 'mrflip/home_chimpy:latest', 'kind': 'data' }

  services:

    # # ===========================================================================
    # #
    # # Helper images
    # #
    # hostname_bot:
    #   doc:         'Manages /etc/hosts on all containers, allowing discovery'
    #   image_name:  'docker_hosts'
    #   entry_args:  ['/srv/hostname_bot/hosts']
    #   volumes:
    #     - '/var/lib/hostname_bot:/srv/hostname_bot'
    #     - '/var/run/docker.sock:/var/run/docker.sock'
    #   hostname:     'hostname_bot'
    #   tags:         ['helper']

    es:
      doc:         'Elasticsearch server for Lipstick'
      image_name:  'elasticsearch'
      hostname:    'es'
      ports:       [ '9200~api', '9300~transport' ]
      tags:         ['helper']

    # ===========================================================================
    #
    # Data
    #

    data_hue:    { volumes: ['/bulk/hadoop/hue'],  tags: ['hue'],    entry_args: ['pushpull', '/bulk/hadoop/hue'],  image_name: 'data_hue', _type: 'rucker.manifest.data_container'    }
    data_hdfs0:  { volumes: ['/bulk/hadoop/hdfs'], tags: ['worker'], entry_args: ['pushpull', '/bulk/hadoop/hdfs'], image_name: 'data_hdfs0', _type: 'rucker.manifest.data_container'  }
    data_gold:   { volumes: ['/data/gold'],        tags: ['lounge'], entry_args: ['pushpull', '/data/gold'],        image_name: 'data_gold', _type: 'rucker.manifest.data_container'   }
    data_outd:   { volumes: ['/data/outd'],        tags: ['lounge'], entry_args: ['pushpull', '/data/outd'],        image_name: 'data_outd', _type: 'rucker.manifest.data_container'   }
    data_nn:     { volumes: ['/bulk/hadoop/name'], tags: ['lounge'], entry_args: ['pushpull', '/bulk/hadoop/name'], image_name: 'data_nn', _type: 'rucker.manifest.data_container'     }
    home_chimpy: { volumes: ['/home/chimpy'],      tags: ['lounge'], entry_args: ['pushpull', '/home/chimpy'],      image_name: 'home_chimpy', _type: 'rucker.manifest.data_container' }

    # ===========================================================================
    #
    # Hadoop machines
    #
    # Remember that these must be in order you want them started
    #

    nn:
      image_name:   'hadoop_nn'
      hostname:     'nn'
      # console http 50070; ipc 8020
      ports:         [ '9522:22~ssh', '50070:50070~view', '8020:8020~ipc' ]
      volumes_from:
        - data_nn
      # links:
      #   - 'hostname_bot:hostname_bot'
      volumes:
         # dynamic /etc/hosts file
        - '/var/lib/hostname_bot/hosts:/etc/hosts:ro'
        - '/bulk/hadoop/log:/bulk/hadoop/log:rw'

    snn:
      image_name: 'hadoop_snn'
      hostname:     'snn'
      # checkpoint api http 50090
      ports:         [ '9422:22~ssh', '50090~ipc' ]
      links:
        - 'nn:nn'
      volumes:
        - '/var/lib/hostname_bot/hosts:/etc/hosts:ro'
        - '/bulk/hadoop/log:/bulk/hadoop/log:rw'

    rm:
      image_name: 'hadoop_rm'
      hostname:     'rm'
      # rsrc mgr: console http 8088; scheduler 8030; tracker 8031; ipc 8032; admin 8033, shuffle 13562;
      # history:  console 19888 / https 19890, ipc 10020, admin 10033
      ports:         [ '9322:22~ssh', '8088:8088~view', '8030:8030~sched', '8031:8031~tracker', '8032:8032~ipc', '8033:8033~yarnad', '13562:13562~shuffle', '19888:19888~histview', '10020:10020~ipc', '10033:10033~histad' ]
      links:
        - 'nn:nn'
      volumes:
        - '/var/lib/hostname_bot/hosts:/etc/hosts:ro'
        - '/bulk/hadoop/log:/bulk/hadoop/log:rw'

    # jt:
    #   image_name: 'hadoop_jt'
    #   hostname:     'jt'
    #   #
    #   ports:         [ '9222:22~ssh', '50030:50030~view', '8021:8021~ipc', '9290:9290~thrift' ]
    #   links:
    #     - 'nn:nn'
    #   volumes:
    #     - '/var/lib/hostname_bot/hosts:/etc/hosts:ro'
    #     - '/bulk/hadoop/log:/bulk/hadoop/log:rw'

    # Give explicit port mappings to this hero worker.
    worker00:
      image_name: 'hadoop_worker'
      # node mgr:  console http 8042, localizer 8040, ipc 8041
      # data node: console http 50075, ipc 50020, xceiver 50010
      ports:         [ '9122:22~ssh', '8042:8042~nmview', '50075:50075~dnview', '50060:50060~tt', '8040~lclzr', '8041~nmipc', '50020~dnipc', '50010~dnxceiver' ]
      links:
        - 'nn:nn'
        - 'rm:rm'
        # - 'jt:jt'
      volumes:
        - '/var/lib/hostname_bot/hosts:/etc/hosts:ro'
        - '/bulk/hadoop/log:/bulk/hadoop/log:rw'
      volumes_from:
        - 'data_hdfs0'
      hostname:     'worker00'

    # All other workers expose their ports on auto-assigned numbers
    grunt:
      image_name: 'hadoop_worker'
      # node mgr:  console http 8042, localizer 8040, ipc 8041
      # data node: console http 50075, ipc 50020, xceiver 50010
      ports:         [ '22~ssh', '8042~nmview', '50075~dnview', '50060~tt', '8040~lclzr', '8041~nmipc', '50020~dnipc', '50010~dnxceiver' ]
      links:
        - 'nn:nn'
        - 'rm:rm'
        # - 'jt:jt'
      volumes:
        - '/var/lib/hostname_bot/hosts:/etc/hosts:ro'

    #
    hue:
      image_name: 'hadoop_hue'
      hostname:     'hue'
      ports:         [ '10002:10002~hue', '9622:22~ssh' ]
      volumes:
        - '/var/lib/hostname_bot/hosts:/etc/hosts:ro'
        - '/bulk/hadoop/log:/bulk/hadoop/log:rw'
        - '/Users:/Users:rw'
      volumes_from:
        - 'data_hue'
        - 'home_chimpy'
      links:
        - 'nn:nn'
        - 'rm:rm'
        # - 'lipstick:lipstick'
        # - 'jt:jt'

    #
    # Running hue on the lounge so that there's only one URL users need to learn.
    #
    lounge:
      image_name: 'hadoop_lounge'
      hostname:     'lounge'
      ports:         [ '9022:22~ssh' ]
      volumes:
        - '/var/lib/hostname_bot/hosts:/etc/hosts:ro'
        - '/bulk/hadoop/log:/bulk/hadoop/log:rw'
        - '/Users:/Users:rw'
      volumes_from:
        - 'data_gold'
        - 'data_outd'
        - 'home_chimpy'
      links:
        - 'nn:nn'
        - 'rm:rm'
        - 'hue:hue'
        # - 'lipstick:lipstick'
        # - 'jt:jt'

    # lipstick:
    #   doc:         'Lipstick server for pig jobs pretty'
    #   image_name:  'lipstick'
    #   hostname:    'lipstick'
    #   ports:       [ '10001:9292~ui' ]
    #   links:       [ 'es:es' ]
    #   volumes_from:
    #     - home_chimpy


  # extra ports to open up to allow expansion
  extra_ports:
    - 9822~ssh
    - 9722~ssh
    - 10002~util
    - 10003~util
    - 10004~util
    - 10005~util
    - 10006~util
    - 10007~util
    - 10008~util
    - 10009~util
