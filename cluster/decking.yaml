# Images decking should create. Use these labels for the 'decking build'
# command, or use 'all' to run all of them.
images: {}

# Containers to define. Options here correspond to common 'docker run' options.
containers:

  # ===========================================================================
  #
  # Data
  #

  data_gold:  { mount: ["/data/gold"],        extra: "pushpull /data/gold",        image: "bd4c/data_gold"  }
  data_outd:  { mount: ["/data/outd"],        extra: "pushpull /data/outd",        image: "bd4c/data_outd"  }
  
  data_nn:    { mount: ["/bulk/hadoop/hue"],  extra: "pushpull /bulk/hadoop/hue",  image: "bd4c/data_hue"    }
  data_nn:    { mount: ["/bulk/hadoop/name"], extra: "pushpull /bulk/hadoop/name", image: "bd4c/data_nn"    }
  data_hdfs0: { mount: ["/bulk/hadoop/hdfs"], extra: "pushpull /bulk/hadoop/hdfs", image: "bd4c/data_hdfs0" }

  home_chimpy: { mount: ["/home/chimpy"],     extra: "pushpull /bulk/hadoop/hdfs", image: "bd4c/home_chimpy" }
  
  # ===========================================================================
  #
  # Hadoop machines
  #

  hadoop_base:    "bd4c/hadoop_base"

  #
  # Running hue on the lounge so that there's only one URL users need to learn.
  #
  hadoop_lounge:
    image:        "bd4c/hadoop_lounge"
    hostname:     "lounge"
    port:         [ "9022:22", "9001:9001" ]
    mount:
      - "/tmp/bulk/hadoop/log:/bulk/hadoop/log:rw"
    mount-from:
      - "data_gold"
      - "data_outd"
      - "home_chimpy"
      - "data_hue"
    dependencies:
      - "hadoop_nn:nn"
      - "hadoop_rm:nn"
      - "hadoop_worker:worker"

  #
  # TODO: We have hardcoded port mappings to the docker host, which is
  # convenient. It also however forces us to have a one-worker-only cluster.
  # Make it be one hero worker and a pool of 0..n anonymous workers (or just a
  # sea of anonymous workers)
  #
  hadoop_worker:
    image:        "bd4c/hadoop_worker"
    # node mgr:  console http 8042, localizer 8040, ipc 8041
    # data node: console http 50075, ipc 50020, xceiver 50010
    port:         [ "9122:22", "8042:8042", "50075:50075", 8040, 8041, 50020, 50010 ]
    dependencies:
      - "hadoop_nn:nn"
      - "hadoop_rm:rm"
    mount:
      - "/var/lib/docker/hosts:/etc/hosts:ro"
      - "/tmp/bulk/hadoop/log:/bulk/hadoop/log:rw"
    mount-from:
      - "data_hdfs0"
    hostname:     "worker"

  hadoop_rm:
    image:        "bd4c/hadoop_rm"
    hostname:     "rm"
    # rsrc mgr: console http 8088; scheduler 8030; tracker 8031; ipc 8032; admin 8033, shuffle 13562;
    # history:  console 19888 / https 19890, ipc 10020, admin 10033
    port:         [ "9322:22", "8088:8088", "8030:8030", "8031:8031", "8032:8032", "8033:8033", "13562:13562", "19888:19888", "10020:10020", "10033:10033" ]
    dependencies:
      - "hadoop_nn:nn"
    mount:
      - "/var/lib/docker/hosts:/etc/hosts:ro"
      - "/tmp/bulk/hadoop/log:/bulk/hadoop/log:rw"

  hadoop_nn:
    image:        "bd4c/hadoop_nn"
    hostname:     "nn"
    # console http 50070; ipc 8020
    port:         [ "9422:22", "50070:50070", "8020:8020" ]
    mount-from:
      - data_nn
    mount:
       # dynamic /etc/hosts file
      - "/var/lib/docker/hosts:/etc/hosts:ro"
      - "/tmp/bulk/hadoop/log:/bulk/hadoop/log:rw"

  hadoop_snn:
    image:        "bd4c/hadoop_snn"
    hostname:     "snn"
    # checkpoint api http 50090
    port:         [ "9522:22", 50090 ]
    dependencies:
      - "hadoop_nn:nn"
    mount:
      - "/var/lib/docker/hosts:/etc/hosts:ro"
      - "/tmp/bulk/hadoop/log:/bulk/hadoop/log:rw"

  # hadoop_solo:
  #   image:        "bd4c/hadoop_solo"
  #   # node mgr:  console http 8042, localizer 8040, ipc 8041
  #   # data node: console http 50075, ipc 50020, xceiver 50010
  #   # 8040, 8041, 50020, 50010, 8030, 8031, 8032, 8033, 13562, 10020, 10033, 8020, 50090
  #   port:         [ "9922:22", "9001:9001", "9002:50070", "9003:8088", "9004:8042", "9905:50075", "9906:19888"  ]
  #   mount:
  #     - "/tmp/solo/hadoop/log:/bulk/hadoop/log:rw"
  #   hostname:     "solo"

  #
  # Helper images
  #

  host_filer:
    image:       "blalor/docker-hosts"
    extra:       "/srv/hosts" # --domain-name=mrflip
    mount:
      - "/var/lib/docker/hosts:/srv/hosts"
      - "/var/run/docker.sock:/var/run/docker.sock"
    hostname:     "host-filer"
  
  # ===========================================================================
  #
  # Debugging
  #
  
  fiddle:
    image:        "bd4c/hadoop_base"
    hostname:     "fiddle"
    port:         [ "9622:22" ]
    mount-from:
      - "data_gold"
  
clusters:
  data:
    - data_nn
    - data_hdfs0
    - data_gold
    - data_outd
  cluster:
    - hadoop_nn
    - hadoop_rm
    - hadoop_snn
    - hadoop_worker
    - hadoop_lounge
  helpers:
    - host_filer
  admin:
    - fiddle
  # han:
  #   - hadoop_solo

groups: {}
